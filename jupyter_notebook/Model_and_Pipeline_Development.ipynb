{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8adf0c-2295-4e1d-872e-06bd38ab8497",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd253f03-1372-4cf8-b1ca-e57434b886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Go to project root: .../Scrabble\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)  # ðŸ‘ˆ Add root, not src!\n",
    "\n",
    "from src.constants import GAMES_FILE_PATH, TURNS_FILE_PATH, TRAIN_FILE_PATH\n",
    "from src.utils import create_dataset, evaluate_multiple_models, tune_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740337f7-9c6c-440d-b7dc-00a0ed34d8e7",
   "metadata": {},
   "source": [
    "## Step 1: Create Training+Validation/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00f1ccb-bb27-4614-b811-10d31edc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features and build one df to contains X_train_val, y_train_val and X_test \n",
    "dataset = create_dataset()\n",
    "\n",
    "# Filter out rows of testing data (rows with user_rating == 0)\n",
    "training_examples = dataset[dataset['user_rating'] != 0]\n",
    "\n",
    "X_train_val = training_examples.drop(columns=['user_rating'])  # Traning + validation features df\n",
    "y_train_val = training_examples['user_rating']  # Train + validation target vector\n",
    "\n",
    "# Extract rows of testing data\n",
    "testing_examples = dataset[dataset['user_rating'] == 0]\n",
    "X_test = testing_examples.drop(columns=['user_rating']) # Test features df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df83703-5763-44c0-9971-b3b1e5d3c6a7",
   "metadata": {},
   "source": [
    "## Step 2: Tune the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cec703-0981-4896-af88-f2f84ca9a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 14:47:26,346] A new study created in memory with name: no-name-2beef31a-d1a7-44fa-8ac0-75518daea67b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing: Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 14:48:11,066] Trial 0 finished with value: 135.63121091356007 and parameters: {'n_estimators': 250, 'max_depth': 48, 'min_samples_split': 15}. Best is trial 0 with value: 135.63121091356007.\n",
      "[I 2025-04-29 14:49:14,264] Trial 1 finished with value: 134.67584752763474 and parameters: {'n_estimators': 340, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:49:55,489] Trial 2 finished with value: 135.88986567769865 and parameters: {'n_estimators': 123, 'max_depth': 44, 'min_samples_split': 13}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:50:38,755] Trial 3 finished with value: 140.7234494851781 and parameters: {'n_estimators': 383, 'max_depth': 5, 'min_samples_split': 20}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:52:02,548] Trial 4 finished with value: 134.777724966935 and parameters: {'n_estimators': 433, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:52:53,725] Trial 5 finished with value: 135.29102717057552 and parameters: {'n_estimators': 173, 'max_depth': 18, 'min_samples_split': 11}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:54:10,575] Trial 6 finished with value: 135.14410578472967 and parameters: {'n_estimators': 273, 'max_depth': 18, 'min_samples_split': 13}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:55:02,119] Trial 7 finished with value: 135.4010009270181 and parameters: {'n_estimators': 155, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:56:22,984] Trial 8 finished with value: 136.30765610337681 and parameters: {'n_estimators': 282, 'max_depth': 41, 'min_samples_split': 5}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:59:31,801] Trial 9 finished with value: 136.45342119708988 and parameters: {'n_estimators': 306, 'max_depth': 32, 'min_samples_split': 2}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-04-29 14:59:31,806] A new study created in memory with name: no-name-54e8ce89-01ab-4109-b1a7-1546d52508b4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for Random Forest: 134.6758\n",
      "\n",
      "Best params: {'n_estimators': 340, 'max_depth': 12, 'min_samples_split': 4}\n",
      "\n",
      "Optimizing: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 14:59:56,977] Trial 0 finished with value: 142.13113403320312 and parameters: {'n_estimators': 250, 'max_depth': 12, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183}. Best is trial 0 with value: 142.13113403320312.\n",
      "[I 2025-04-29 14:59:59,996] Trial 1 finished with value: 141.61570739746094 and parameters: {'n_estimators': 162, 'max_depth': 4, 'learning_rate': 0.012184186502221764, 'subsample': 0.9330880728874675}. Best is trial 1 with value: 141.61570739746094.\n",
      "[I 2025-04-29 15:00:19,458] Trial 2 finished with value: 134.92880249023438 and parameters: {'n_estimators': 341, 'max_depth': 10, 'learning_rate': 0.010725209743171996, 'subsample': 0.9849549260809971}. Best is trial 2 with value: 134.92880249023438.\n",
      "[I 2025-04-29 15:00:26,660] Trial 3 finished with value: 134.5030975341797 and parameters: {'n_estimators': 433, 'max_depth': 5, 'learning_rate': 0.01855998084649059, 'subsample': 0.5917022549267169}. Best is trial 3 with value: 134.5030975341797.\n",
      "[I 2025-04-29 15:00:33,634] Trial 4 finished with value: 134.39043579101562 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'subsample': 0.645614570099021}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:00:39,064] Trial 5 finished with value: 135.15639038085936 and parameters: {'n_estimators': 345, 'max_depth': 4, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:00:54,179] Trial 6 finished with value: 134.6245147705078 and parameters: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'subsample': 0.7571172192068059}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:00:58,456] Trial 7 finished with value: 135.43631286621093 and parameters: {'n_estimators': 337, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'subsample': 0.5852620618436457}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:01:10,964] Trial 8 finished with value: 146.3693817138672 and parameters: {'n_estimators': 126, 'max_depth': 12, 'learning_rate': 0.26690431824362526, 'subsample': 0.9041986740582306}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:01:13,971] Trial 9 finished with value: 135.5564178466797 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.1024932221692416, 'subsample': 0.7200762468698007}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-04-29 15:01:13,979] A new study created in memory with name: no-name-3655635d-80fb-4283-8947-8e7339378a48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for XGBoost: 134.3904\n",
      "\n",
      "Best params: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'subsample': 0.645614570099021}\n",
      "\n",
      "Optimizing: LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:01:37,678] Trial 0 finished with value: 137.05493571117086 and parameters: {'n_estimators': 250, 'max_depth': 12, 'learning_rate': 0.1205712628744377, 'num_leaves': 98}. Best is trial 0 with value: 137.05493571117086.\n",
      "[I 2025-04-29 15:01:42,809] Trial 1 finished with value: 141.64432684343515 and parameters: {'n_estimators': 162, 'max_depth': 4, 'learning_rate': 0.012184186502221764, 'num_leaves': 133}. Best is trial 0 with value: 137.05493571117086.\n",
      "[I 2025-04-29 15:02:36,544] Trial 2 finished with value: 134.426827202161 and parameters: {'n_estimators': 341, 'max_depth': 10, 'learning_rate': 0.010725209743171996, 'num_leaves': 147}. Best is trial 2 with value: 134.426827202161.\n",
      "[I 2025-04-29 15:02:52,243] Trial 3 finished with value: 134.74236740646762 and parameters: {'n_estimators': 433, 'max_depth': 5, 'learning_rate': 0.01855998084649059, 'num_leaves': 44}. Best is trial 2 with value: 134.426827202161.\n",
      "[I 2025-04-29 15:03:06,728] Trial 4 finished with value: 134.30216391195924 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'num_leaves': 58}. Best is trial 4 with value: 134.30216391195924.\n",
      "[I 2025-04-29 15:03:14,957] Trial 5 finished with value: 135.39218125232242 and parameters: {'n_estimators': 345, 'max_depth': 4, 'learning_rate': 0.027010527749605478, 'num_leaves': 67}. Best is trial 4 with value: 134.30216391195924.\n",
      "[I 2025-04-29 15:03:43,046] Trial 6 finished with value: 134.15418875205 and parameters: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'num_leaves': 87}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-04-29 15:03:48,615] Trial 7 finished with value: 135.60600601650987 and parameters: {'n_estimators': 337, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'num_leaves': 42}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-04-29 15:04:03,338] Trial 8 finished with value: 140.30348047378018 and parameters: {'n_estimators': 126, 'max_depth': 12, 'learning_rate': 0.26690431824362526, 'num_leaves': 125}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-04-29 15:04:07,154] Trial 9 finished with value: 135.70418357947045 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.1024932221692416, 'num_leaves': 77}. Best is trial 6 with value: 134.15418875205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for LightGBM: 134.1542\n",
      "\n",
      "Best params: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'num_leaves': 87}\n",
      "{'Random Forest': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexicon'])])),\n",
      "                ('model',\n",
      "                 RandomForestRegressor(max_depth=12, min_samples_split=4,\n",
      "                                       n_estimators=340, n_jobs=-1,\n",
      "                                       random_state=42))]), 'XGBoost': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexi...\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None,\n",
      "                              learning_rate=0.04345454109729477, max_bin=None,\n",
      "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                              max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=222, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))]), 'LightGBM': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexicon'])])),\n",
      "                ('model',\n",
      "                 LGBMRegressor(learning_rate=0.019721610970574007, max_depth=10,\n",
      "                               n_estimators=282, n_jobs=-1, num_leaves=87,\n",
      "                               random_state=42))])}\n",
      "           model  Mean_CV_RMSE\n",
      "0       LightGBM    134.154189\n",
      "1        XGBoost    134.390436\n",
      "2  Random Forest    134.675848\n"
     ]
    }
   ],
   "source": [
    "# Tune all models\n",
    "cross_val_scores_df, tuned_models = tune_all_models(X_train_val, y_train_val)\n",
    "print(cross_val_scores_df, end='\\n')\n",
    "print(tuned_models, end='\\n')\n",
    "best_model = tuned_models[cross_val_scores_df.iloc[0]['model']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
