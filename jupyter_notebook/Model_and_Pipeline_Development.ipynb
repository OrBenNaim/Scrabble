{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8adf0c-2295-4e1d-872e-06bd38ab8497",
   "metadata": {},
   "source": [
    "# Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd253f03-1372-4cf8-b1ca-e57434b886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Go to project root: .../Scrabble\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)  # Add root\n",
    "\n",
    "from src.utils import create_dataset, find_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740337f7-9c6c-440d-b7dc-00a0ed34d8e7",
   "metadata": {},
   "source": [
    "# Step 1: Create Training+Validation/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00f1ccb-bb27-4614-b811-10d31edc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features and build one df to contains X_train_val, y_train_val and X_test \n",
    "dataset = create_dataset()\n",
    "\n",
    "# Filter out rows of testing data (rows with user_rating == 0)\n",
    "training_examples = dataset[dataset['user_rating'] != 0]\n",
    "\n",
    "X_train_val = training_examples.drop(columns=['user_rating'])  # Traning + validation features df\n",
    "y_train_val = training_examples['user_rating']  # Train + validation target vector\n",
    "\n",
    "# Extract rows of testing data\n",
    "testing_examples = dataset[dataset['user_rating'] == 0]\n",
    "X_test = testing_examples.drop(columns=['user_rating']) # Test features df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df83703-5763-44c0-9971-b3b1e5d3c6a7",
   "metadata": {},
   "source": [
    "# Step 2: Find the best model after tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cec703-0981-4896-af88-f2f84ca9a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 11:36:09,838] A new study created in memory with name: no-name-64931b5a-d355-4f9e-8eae-ae82c1761973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing: Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 11:37:13,982] Trial 0 finished with value: 135.63121091356007 and parameters: {'n_estimators': 250, 'max_depth': 48, 'min_samples_split': 15}. Best is trial 0 with value: 135.63121091356007.\n",
      "[I 2025-05-03 11:38:09,277] Trial 1 finished with value: 134.67584752763474 and parameters: {'n_estimators': 340, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:38:40,635] Trial 2 finished with value: 135.88986567769865 and parameters: {'n_estimators': 123, 'max_depth': 44, 'min_samples_split': 13}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:39:07,510] Trial 3 finished with value: 140.7234494851781 and parameters: {'n_estimators': 383, 'max_depth': 5, 'min_samples_split': 20}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:40:26,166] Trial 4 finished with value: 134.777724966935 and parameters: {'n_estimators': 433, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:41:04,273] Trial 5 finished with value: 135.29102717057555 and parameters: {'n_estimators': 173, 'max_depth': 18, 'min_samples_split': 11}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:41:59,476] Trial 6 finished with value: 135.14410578472967 and parameters: {'n_estimators': 273, 'max_depth': 18, 'min_samples_split': 13}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:42:33,256] Trial 7 finished with value: 135.4010009270181 and parameters: {'n_estimators': 155, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:43:53,839] Trial 8 finished with value: 136.30765610337681 and parameters: {'n_estimators': 282, 'max_depth': 41, 'min_samples_split': 5}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:45:30,330] Trial 9 finished with value: 136.45342119708988 and parameters: {'n_estimators': 306, 'max_depth': 32, 'min_samples_split': 2}. Best is trial 1 with value: 134.67584752763474.\n",
      "[I 2025-05-03 11:45:30,330] A new study created in memory with name: no-name-3c58e344-712f-4844-8a4e-764076ff383b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for Random Forest: 134.6758\n",
      "\n",
      "Best params: {'n_estimators': 340, 'max_depth': 12, 'min_samples_split': 4}\n",
      "\n",
      "Optimizing: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 11:45:51,113] Trial 0 finished with value: 142.13113403320312 and parameters: {'n_estimators': 250, 'max_depth': 12, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183}. Best is trial 0 with value: 142.13113403320312.\n",
      "[I 2025-05-03 11:45:54,527] Trial 1 finished with value: 141.61570739746094 and parameters: {'n_estimators': 162, 'max_depth': 4, 'learning_rate': 0.012184186502221764, 'subsample': 0.9330880728874675}. Best is trial 1 with value: 141.61570739746094.\n",
      "[I 2025-05-03 11:46:16,004] Trial 2 finished with value: 134.92880249023438 and parameters: {'n_estimators': 341, 'max_depth': 10, 'learning_rate': 0.010725209743171996, 'subsample': 0.9849549260809971}. Best is trial 2 with value: 134.92880249023438.\n",
      "[I 2025-05-03 11:46:22,723] Trial 3 finished with value: 134.5030975341797 and parameters: {'n_estimators': 433, 'max_depth': 5, 'learning_rate': 0.01855998084649059, 'subsample': 0.5917022549267169}. Best is trial 3 with value: 134.5030975341797.\n",
      "[I 2025-05-03 11:46:28,226] Trial 4 finished with value: 134.39043579101562 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'subsample': 0.645614570099021}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:46:32,998] Trial 5 finished with value: 135.15639038085936 and parameters: {'n_estimators': 345, 'max_depth': 4, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:46:48,141] Trial 6 finished with value: 134.6245147705078 and parameters: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'subsample': 0.7571172192068059}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:46:52,313] Trial 7 finished with value: 135.43631286621093 and parameters: {'n_estimators': 337, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'subsample': 0.5852620618436457}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:47:04,135] Trial 8 finished with value: 146.3693817138672 and parameters: {'n_estimators': 126, 'max_depth': 12, 'learning_rate': 0.26690431824362526, 'subsample': 0.9041986740582306}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:47:07,164] Trial 9 finished with value: 135.5564178466797 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.1024932221692416, 'subsample': 0.7200762468698007}. Best is trial 4 with value: 134.39043579101562.\n",
      "[I 2025-05-03 11:47:07,164] A new study created in memory with name: no-name-68690595-ee27-4cc2-a913-5ecd62b65e0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for XGBoost: 134.3904\n",
      "\n",
      "Best params: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'subsample': 0.645614570099021}\n",
      "\n",
      "Optimizing: LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 11:47:27,283] Trial 0 finished with value: 137.05493571117086 and parameters: {'n_estimators': 250, 'max_depth': 12, 'learning_rate': 0.1205712628744377, 'num_leaves': 98}. Best is trial 0 with value: 137.05493571117086.\n",
      "[I 2025-05-03 11:47:30,322] Trial 1 finished with value: 141.64432684343515 and parameters: {'n_estimators': 162, 'max_depth': 4, 'learning_rate': 0.012184186502221764, 'num_leaves': 133}. Best is trial 0 with value: 137.05493571117086.\n",
      "[I 2025-05-03 11:48:01,975] Trial 2 finished with value: 134.426827202161 and parameters: {'n_estimators': 341, 'max_depth': 10, 'learning_rate': 0.010725209743171996, 'num_leaves': 147}. Best is trial 2 with value: 134.426827202161.\n",
      "[I 2025-05-03 11:48:09,438] Trial 3 finished with value: 134.74236740646762 and parameters: {'n_estimators': 433, 'max_depth': 5, 'learning_rate': 0.01855998084649059, 'num_leaves': 44}. Best is trial 2 with value: 134.426827202161.\n",
      "[I 2025-05-03 11:48:16,888] Trial 4 finished with value: 134.30216391195924 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'num_leaves': 58}. Best is trial 4 with value: 134.30216391195924.\n",
      "[I 2025-05-03 11:48:20,339] Trial 5 finished with value: 135.39218125232242 and parameters: {'n_estimators': 345, 'max_depth': 4, 'learning_rate': 0.027010527749605478, 'num_leaves': 67}. Best is trial 4 with value: 134.30216391195924.\n",
      "[I 2025-05-03 11:48:35,147] Trial 6 finished with value: 134.15418875205 and parameters: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'num_leaves': 87}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-05-03 11:48:37,621] Trial 7 finished with value: 135.60600601650987 and parameters: {'n_estimators': 337, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'num_leaves': 42}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-05-03 11:48:45,630] Trial 8 finished with value: 140.30348047378018 and parameters: {'n_estimators': 126, 'max_depth': 12, 'learning_rate': 0.26690431824362526, 'num_leaves': 125}. Best is trial 6 with value: 134.15418875205.\n",
      "[I 2025-05-03 11:48:47,373] Trial 9 finished with value: 135.70418357947045 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.1024932221692416, 'num_leaves': 77}. Best is trial 6 with value: 134.15418875205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for LightGBM: 134.1542\n",
      "\n",
      "Best params: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'num_leaves': 87}\n",
      "           model  Mean_CV_RMSE\n",
      "0       LightGBM    134.154189\n",
      "1        XGBoost    134.390436\n",
      "2  Random Forest    134.675848\n",
      "{'Random Forest': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexicon'])])),\n",
      "                ('model',\n",
      "                 RandomForestRegressor(max_depth=12, min_samples_split=4,\n",
      "                                       n_estimators=340, n_jobs=-1,\n",
      "                                       random_state=42))]), 'XGBoost': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexi...\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None,\n",
      "                              learning_rate=0.04345454109729477, max_bin=None,\n",
      "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                              max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=222, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))]), 'LightGBM': Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexicon'])])),\n",
      "                ('model',\n",
      "                 LGBMRegressor(learning_rate=0.019721610970574007, max_depth=10,\n",
      "                               n_estimators=282, n_jobs=-1, num_leaves=87,\n",
      "                               random_state=42))])}\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  ['avg_word_length',\n",
      "                                                   'bingo_count',\n",
      "                                                   'hard_letter_plays',\n",
      "                                                   'negative_turns_count',\n",
      "                                                   'pass_count',\n",
      "                                                   'exchange_count',\n",
      "                                                   'user_score',\n",
      "                                                   'avg_extra_points_per_turn',\n",
      "                                                   'bot_score', 'bot_rating',\n",
      "                                                   'bot_level']),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['lexicon'])])),\n",
      "                ('model',\n",
      "                 LGBMRegressor(learning_rate=0.019721610970574007, max_depth=10,\n",
      "                               n_estimators=282, n_jobs=-1, num_leaves=87,\n",
      "                               random_state=42))])\n",
      "RMSE on the training set = 123.689\n",
      "RMSE on the validation set = 134.154\n",
      "Train RMSE: 123.689\n",
      "Validation RMSE: 134.154\n",
      "Generalization Gap: 8.461\n"
     ]
    }
   ],
   "source": [
    "best_model, avg_train_rmse, avg_val_rmse = find_best_model(X_train_val, y_train_val)\n",
    "\n",
    "print(best_model, end='\\n')\n",
    "print(f\"RMSE on the training set = {avg_train_rmse:.3f}\", end='\\n')\n",
    "print(f\"RMSE on the validation set = {avg_val_rmse:.3f}\")\n",
    "\n",
    "print(f\"Train RMSE: {avg_train_rmse:.3f}\")\n",
    "print(f\"Validation RMSE: {avg_val_rmse:.3f}\")\n",
    "\n",
    "gap = ((avg_val_rmse / avg_train_rmse) - 1) * 100\n",
    "print(f\"Generalization Gap: {gap:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
